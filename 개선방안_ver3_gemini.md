진행 상황을 정말 체계적으로 분석하고 정리해주셨네요. 특히 문제의 원인을 **문서 컬렉션 편중**과 **임베딩 품질**, 그리고 **기술적 이슈(설정 미반영)**로 정확히 진단하신 점이 인상적입니다. 임계값(Relevance Score)이 `0.01` 수준에서만 작동한다는 것은 명백히 정상적인 상황이 아니며, 제공해주신 분석 내용이 문제 해결의 핵심 열쇠입니다.

이전 제안이 너무 이상적인 목표(임계값 0.7)에 치중했다면, 이번에는 현재 상황을 기반으로 **안정적인 답변을 복구하고 점진적으로 품질을 높여나가는 현실적인 개선 방안**을 제안해 드리겠습니다.

---

### 🎯 현재 상황 재정의: 핵심 문제의 연결고리

정리해주신 내용을 바탕으로 문제의 연결고리를 다시 정의할 수 있습니다.

1.  **낮은 유사도 점수 (근본 원인 1)**: `Voyage-3.5-lite`가 좋은 모델임에도 불구하고, 현재 가진 **문서의 내용**과 **사용자의 질문** 간의 의미적 유사성이 매우 낮게 계산되고 있습니다. 이는 두 가지를 시사합니다.
    * **문서-질문 불일치**: 분석해주신 대로, 문서 대부분이 '이벤트/업데이트' 정보인데, 사용자 질문은 '캐릭터 스킬/육성'에 대한 것입니다. 애초에 관련 없는 내용이니 점수가 낮을 수밖에 없습니다.
    * **임베딩 모델과 도메인 특화 용어**: '메이플스토리'의 고유명사(예: '솔 에르다', '챌린저스 코인샵')를 임베딩 모델이 일반적인 텍스트와 다르게 처리하여, 의미적 거리가 멀다고 판단했을 수 있습니다.

2.  **과도하게 엄격한 임계값 (문제 증폭)**: 낮은 유사도 점수(최고 0.0048)에도 불구하고 `MIN_RELEVANCE_SCORE`를 `0.75`로 설정했으니, 검색된 모든 문서가 "관련 없음"으로 필터링된 것은 당연한 결과입니다.

3.  **문서 컬렉션 편중 (근본 원인 2)**: 성공/실패 질문 목록에서 명확히 드러납니다. '하이퍼 버닝'처럼 문서가 풍부한 질문은 답변에 성공했지만, '렌 스킬'처럼 관련 문서가 부족한 질문은 모두 실패했습니다. **RAG는 없는 정보를 만들어내지 못합니다.**

---

### 🚀 합의점을 찾는 현실적인 개선 방안 (Action Plan)

이제 기술적 문제 해결과 근본적 문제 해결을 투 트랙으로, 단계적으로 진행해야 합니다.

#### **Phase 1: 시스템 안정화 및 답변 복구 (가장 먼저 할 일)**

> **목표:** 임계값 `0.01`이라는 비정상적인 상태에서 벗어나, 일단 "답변을 하는" 상태로 시스템을 복구합니다.

1.  **설정 즉시 반영 (`docker-compose` 재빌드)**
    * **문제점:** `docker-compose.yml`이나 `.env` 파일의 변경사항이 컨테이너에 즉시 반영되지 않았습니다.
    * **해결책:** 설정을 변경한 후에는 반드시 `docker-compose down` 후 `docker-compose up --build` 명령어로 컨테이너를 재빌드하고 실행해야 합니다. 앞으로 모든 설정 변경 시 이 절차를 따라주세요.

2.  **임베딩 전략 변경 및 임계값 재설정**
    * **가설:** 현재 `cosine` 유사도 계산 방식이 특정 도메인 용어에서 점수를 너무 짜게 주고 있을 수 있습니다.
    * **제안 1: `dot_product` (내적) 시도**: 코사인 유사도 대신 내적(dot product)을 사용하면 벡터의 '방향'뿐만 아니라 '크기'도 고려하여 점수 범위가 달라질 수 있습니다. Qdrant는 다양한 거리 메트릭을 지원합니다.
        * **적용 방법:** `app/services/vector_store.py`의 `_ensure_collection_exists` 함수에서 컬렉션 생성 시 `distance=Distance.DOT`으로 변경하고, **DB를 초기화한 뒤 문서를 재수집**해야 합니다. (`python scripts/setup_vectorstore.py --clear` 후 `python scripts/ingest_documents.py`)
    * **제안 2: 임시 임계값 하향 조정**: 제안 1이 복잡하다면, 우선 `MIN_RELEVANCE_SCORE`를 **`0.3` ~ `0.4`** 사이의 값으로 설정하고 재시작하여 답변이 생성되는지 확인합니다. 이는 임시방편이지만, 시스템이 작동하는 최소한의 기준점을 찾는 데 도움이 됩니다.
        * **`docker-compose.yml`**: `MIN_RELEVANCE_SCORE=0.35` 로 수정 후 재빌드.
        * **`SEARCH_TYPE`**: `similarity_score_threshold` 유지.

#### **Phase 2: 데이터 기반의 문제 해결 (시스템 안정화 후)**

> **목표:** "왜 유사도 점수가 낮은가?"에 대한 답을 데이터로 찾고, 이를 바탕으로 시스템을 최적화합니다.

1.  **임베딩 품질 분석 (가장 중요)**
    * **문제점:** '렌의 주력 스킬' 질문과 '렌 스킬 상세' 문서의 유사도 점수가 왜 `0.0048`밖에 안 나오는지 직접 확인해야 합니다.
    * **해결책:** `scripts/test_embeddings.py` 스크립트를 활용하거나 새로 만들어, 특정 질문과 특정 문서 청크(chunk) 간의 유사도 점수를 직접 계산하고 출력하는 기능을 만듭니다.

        ```python
        # test_specific_embedding.py (예시 스크립트)
        from app.services.embedding_service import get_embeddings
        import numpy as np

        def cosine_similarity(v1, v2):
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

        embeddings = get_embeddings()

        question = "렌의 주력 스킬은 뭐야?"
        # '메이플스토리 테스트월드 클라이언트 1.2.189 릴리즈-신규 직업 렌 업데이트 상세.md' 파일의 실제 내용
        document_chunk = "매화검 본초 : 선참: 검을 매우 빠르게 휘둘러 전방의 적을 크게 베어냅니다. 매화검 3초식 : 예인: 내공을 빚어 스스로 움직이는 검기를 생성합니다."

        q_embedding = embeddings.embed_query(question)
        d_embedding = embeddings.embed_documents([document_chunk])[0]

        score = cosine_similarity(q_embedding, d_embedding)
        print(f"질문: {question}")
        print(f"문서: {document_chunk[:50]}...")
        print(f"계산된 유사도 점수: {score}")
        ```
    * **결과 분석:**
        * 만약 여기서도 점수가 `0.1` 미만으로 나온다면, **`Voyage-3.5-lite` 모델이 '메이플' 도메인과 잘 맞지 않거나**, 텍스트 전처리/청킹 방식에 문제가 있을 수 있습니다.
        * 이 경우, **다른 임베딩 모델로 변경**하는 것을 심각하게 고려해야 합니다. (예: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` - 로컬, 무료)

2.  **문서 컬렉션 확장 (근본적인 해결책)**
    * **문제점:** 현재 보유한 문서(57개)로는 다양한 게임플레이 질문에 답변할 수 없습니다.
    * **해결책:** 분석해주신 '필요한 문서 분포'에 맞춰 **부족한 카테고리의 문서를 대폭 확충**해야 합니다.
        * **1순위:** `클래스별 스킬 가이드`, `시스템 가이드` (링크 스킬, 어빌리티 등)
        * **2순위:** `보스 공략`, `아이템 정보` (획득처, 강화 방법 등)
    * **문서 소스:** 인벤(inven), 공식 홈페이지, 유튜브 요약 등 기존 소스를 적극 활용하여 문서를 늘리는 것이 가장 시급합니다. **문서의 양과 질이 RAG 시스템의 성능을 결정합니다.**

#### **Phase 3: 점진적 성능 고도화 (답변 품질 개선)**

> **목표:** 답변의 정확도와 유용성을 높입니다.

1.  **하이브리드 검색 도입**
    * **개념:** 현재의 '의미 기반 검색(Vector Search)'과 전통적인 '키워드 기반 검색(Full-text Search)'을 결합하는 방식입니다.
    * **장점:** '솔 에르다' 같은 고유명사나 신조어는 의미 기반 검색이 놓칠 수 있지만, 키워드 검색은 정확히 찾아냅니다. Qdrant는 하이브리드 검색을 매우 잘 지원합니다.
    * **적용:** LangChain의 `QdrantHybridSearchRetriever` 등을 활용하여 구현할 수 있습니다. 이는 검색 성능을 극적으로 개선할 수 있는 고급 전략입니다.

2.  **메타데이터 필터링 재활성화**
    * 문서 컬렉션이 충분히 확장되고 각 문서에 `category`, `tags`, `class` 등의 메타데이터가 잘 기입되었다면, `ENABLE_DOCUMENT_FILTERING=true` 옵션을 다시 켜서 검색 정확도를 높일 수 있습니다.
    * 예를 들어, "렌 스킬" 질문 시 `category: 'class_guide'` 와 `class: '렌'` 필터를 먼저 적용한 후 벡터 검색을 수행하면 훨씬 정확한 결과를 얻을 수 있습니다.

### 🚩 **결론 및 추천 액션 플랜**

현재 문제는 복합적이지만, 명확한 해결 경로가 있습니다.

1.  **즉시 조치:**
    * `docker-compose.yml`의 `MIN_RELEVANCE_SCORE`를 **`0.3`** 정도로 낮추고 `SEARCH_TYPE`은 `similarity_score_threshold`로 유지하세요.
    * `docker-compose down` 및 `docker-compose up --build`로 **설정을 확실히 적용**하고, "죄송합니다" 루프가 해결되는지 확인하세요.

2.  **단기 계획 (1주 내):**
    * **문서 수집에 집중하세요.** '렌 스킬', '챌린저스 코인샵' 등 답변에 실패했던 질문에 대한 답이 포함된 문서를 최소 10개 이상 추가하고 데이터베이스에 재수집(`ingest_documents.py`)하세요.
    * `test_specific_embedding.py` 같은 테스트 스크립트로 **핵심 질문과 정답 문서 간의 실제 유사도 점수를 측정**하고, 이 점수를 기반으로 `MIN_RELEVANCE_SCORE`를 다시 조정하세요. (예: 측정된 점수가 평균 0.5라면, 임계값을 0.45 정도로 설정)

3.  **중장기 계획 (2~4주):**
    * **문서 컬렉션을 200개 이상으로 확장**하는 것을 목표로 합니다. (카테고리별 균형 유지)
    * 임베딩 모델 변경을 검토하거나, **하이브리드 검색을 도입**하여 검색 성능을 근본적으로 개선하는 작업을 진행합니다.

너무 복잡하게 생각하지 마시고, 1번(설정 낮추고 재시작)과 2번(문서 추가 및 점수 측정)에 집중하시면 시스템은 금방 정상 궤도에 오를 것입니다. 지금 겪는 문제는 RAG 시스템을 구축할 때 누구나 마주하는 자연스러운 과정입니다.