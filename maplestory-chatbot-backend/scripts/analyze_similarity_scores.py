#!/usr/bin/env python3
"""
ìœ ì‚¬ë„ ì ìˆ˜ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ - ì„ê³„ê°’ ìµœì í™”ë¥¼ ìœ„í•œ ë°ì´í„° ë¶„ì„
"""

import asyncio
import sys
import logging
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.services.langchain_service import LangChainService
from app.services.embedding_service import get_embeddings
from app.config.settings import settings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimilarityAnalyzer:
    def __init__(self):
        self.langchain_service = LangChainService()
        self.embeddings = get_embeddings()
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ (ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬)
        self.test_questions = [
            {"question": "ë Œì˜ ì£¼ë ¥ ìŠ¤í‚¬ì€ ë­ì•¼?", "category": "class_skill", "expected_keywords": ["ë Œ", "ìŠ¤í‚¬", "ë§¤í™”ê²€"]},
            {"question": "ì±Œë¦°ì €ìŠ¤ ì½”ì¸ìƒµì—ì„œ ë­˜ ì‚´ ìˆ˜ ìˆì–´?", "category": "system_shop", "expected_keywords": ["ì±Œë¦°ì €ìŠ¤", "ì½”ì¸ìƒµ", "êµ¬ë§¤"]},
            {"question": "í•˜ì´í¼ ë²„ë‹ ì´ë²¤íŠ¸ ë³´ìƒì€?", "category": "event_reward", "expected_keywords": ["í•˜ì´í¼", "ë²„ë‹", "ë³´ìƒ"]},
            {"question": "ì†” ì—ë¥´ë‹¤ëŠ” ì–´ë–»ê²Œ ì–»ì–´?", "category": "item_obtain", "expected_keywords": ["ì†” ì—ë¥´ë‹¤", "íšë“", "ì–»ëŠ”ë²•"]},
            {"question": "ë©”ì´í”Œ íŒ¨ìŠ¤ í˜œíƒì´ ë­ì•¼?", "category": "system_benefit", "expected_keywords": ["ë©”ì´í”Œ íŒ¨ìŠ¤", "í˜œíƒ", "íŠ¹ì „"]},
            {"question": "ë³´ìŠ¤ ëª¬ìŠ¤í„° ì¶”ì²œí•´ì¤˜", "category": "boss_guide", "expected_keywords": ["ë³´ìŠ¤", "ëª¬ìŠ¤í„°", "ì¶”ì²œ"]},
            {"question": "ë§í¬ ìŠ¤í‚¬ ì¶”ì²œ", "category": "character_system", "expected_keywords": ["ë§í¬", "ìŠ¤í‚¬", "ì¶”ì²œ"]},
            {"question": "ì‹ ê·œ ìœ ì € ê°€ì´ë“œ", "category": "beginner_guide", "expected_keywords": ["ì‹ ê·œ", "ì´ˆë³´", "ê°€ì´ë“œ"]},
            {"question": "ë Œ ìœ¡ì„± ê°€ì´ë“œ", "category": "class_guide", "expected_keywords": ["ë Œ", "ìœ¡ì„±", "ê°€ì´ë“œ"]},
            {"question": "ì—¬ë¦„ ì´ë²¤íŠ¸ ì •ë³´", "category": "event_info", "expected_keywords": ["ì—¬ë¦„", "ì´ë²¤íŠ¸", "ì •ë³´"]}
        ]
    
    def cosine_similarity(self, v1: List[float], v2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        v1_np = np.array(v1)
        v2_np = np.array(v2)
        
        dot_product = np.dot(v1_np, v2_np)
        norm_v1 = np.linalg.norm(v1_np)
        norm_v2 = np.linalg.norm(v2_np)
        
        if norm_v1 == 0 or norm_v2 == 0:
            return 0.0
        
        return float(dot_product / (norm_v1 * norm_v2))
    
    async def analyze_question_document_similarity(self) -> Dict:
        """ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ë¶„ì„"""
        logger.info("ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "embedding_model": settings.voyage_model,
            "questions_analyzed": len(self.test_questions),
            "question_results": [],
            "overall_stats": {},
            "recommendations": {}
        }
        
        all_scores = []
        
        for test_q in self.test_questions:
            question = test_q["question"]
            category = test_q["category"]
            expected_keywords = test_q["expected_keywords"]
            
            logger.info(f"ë¶„ì„ ì¤‘: {question}")
            
            # ì§ˆë¬¸ ì„ë² ë”©
            question_embedding = self.embeddings.embed_query(question)
            
            # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìƒìœ„ 10ê°œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            try:
                retriever = self.langchain_service.vector_store.get_retriever(k=10, search_type="similarity")
                documents = retriever.get_relevant_documents(question)
                
                question_result = {
                    "question": question,
                    "category": category,
                    "expected_keywords": expected_keywords,
                    "documents_found": len(documents),
                    "document_scores": [],
                    "max_score": 0.0,
                    "min_score": 1.0,
                    "avg_score": 0.0,
                    "relevant_docs_count": 0
                }
                
                # ê° ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                for i, doc in enumerate(documents):
                    doc_embedding = self.embeddings.embed_documents([doc.page_content])[0]
                    similarity_score = self.cosine_similarity(question_embedding, doc_embedding)
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                    content_lower = doc.page_content.lower()
                    title_lower = doc.metadata.get('title', '').lower()
                    keyword_matches = sum(1 for kw in expected_keywords if kw.lower() in content_lower or kw.lower() in title_lower)
                    is_relevant = keyword_matches >= 1
                    
                    doc_info = {
                        "rank": i + 1,
                        "title": doc.metadata.get('title', 'No Title'),
                        "source": doc.metadata.get('source', ''),
                        "similarity_score": round(similarity_score, 6),
                        "keyword_matches": keyword_matches,
                        "is_relevant": is_relevant,
                        "content_preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                    }
                    
                    question_result["document_scores"].append(doc_info)
                    all_scores.append(similarity_score)
                    
                    if is_relevant:
                        question_result["relevant_docs_count"] += 1
                
                # í†µê³„ ê³„ì‚°
                if question_result["document_scores"]:
                    scores = [d["similarity_score"] for d in question_result["document_scores"]]
                    question_result["max_score"] = max(scores)
                    question_result["min_score"] = min(scores)
                    question_result["avg_score"] = sum(scores) / len(scores)
                
                analysis_results["question_results"].append(question_result)
                
            except Exception as e:
                logger.error(f"ì§ˆë¬¸ '{question}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                question_result["error"] = str(e)
                analysis_results["question_results"].append(question_result)
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        if all_scores:
            analysis_results["overall_stats"] = {
                "total_scores": len(all_scores),
                "max_score": max(all_scores),
                "min_score": min(all_scores),
                "avg_score": sum(all_scores) / len(all_scores),
                "median_score": np.median(all_scores),
                "std_score": np.std(all_scores),
                "percentiles": {
                    "p25": np.percentile(all_scores, 25),
                    "p50": np.percentile(all_scores, 50),
                    "p75": np.percentile(all_scores, 75),
                    "p90": np.percentile(all_scores, 90),
                    "p95": np.percentile(all_scores, 95)
                }
            }
            
            # ì„ê³„ê°’ ì¶”ì²œ
            stats = analysis_results["overall_stats"]
            analysis_results["recommendations"] = {
                "conservative_threshold": round(stats["percentiles"]["p25"], 3),  # ë³´ìˆ˜ì  (25%ile)
                "balanced_threshold": round(stats["percentiles"]["p50"], 3),      # ê· í˜• (50%ile)
                "aggressive_threshold": round(stats["percentiles"]["p75"], 3),    # ê³µê²©ì  (75%ile)
                "current_threshold": settings.min_relevance_score,
                "suggested_threshold": round(max(0.001, stats["percentiles"]["p25"]), 3)
            }
        
        return analysis_results
    
    async def test_threshold_performance(self, thresholds: List[float]) -> Dict:
        """ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info(f"ì„ê³„ê°’ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: {thresholds}")
        
        performance_results = {
            "timestamp": datetime.now().isoformat(),
            "thresholds_tested": thresholds,
            "threshold_results": []
        }
        
        for threshold in thresholds:
            logger.info(f"ì„ê³„ê°’ {threshold} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            threshold_result = {
                "threshold": threshold,
                "questions_answered": 0,
                "questions_total": len(self.test_questions),
                "relevant_answers": 0,
                "question_details": []
            }
            
            for test_q in self.test_questions:
                question = test_q["question"]
                
                try:
                    # ì„ì‹œë¡œ ì„ê³„ê°’ ë³€ê²½í•˜ì—¬ ê²€ìƒ‰
                    original_score = settings.min_relevance_score
                    settings.min_relevance_score = threshold
                    
                    # ê²€ìƒ‰ ì‹¤í–‰
                    retriever = self.langchain_service.vector_store.get_retriever(
                        k=5, 
                        search_type="similarity_score_threshold"
                    )
                    documents = retriever.get_relevant_documents(question)
                    
                    # ì„¤ì • ë³µì›
                    settings.min_relevance_score = original_score
                    
                    answered = len(documents) > 0
                    if answered:
                        threshold_result["questions_answered"] += 1
                        
                        # ê´€ë ¨ì„± í™•ì¸ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
                        content = " ".join([doc.page_content for doc in documents]).lower()
                        keywords_found = sum(1 for kw in test_q["expected_keywords"] if kw.lower() in content)
                        is_relevant = keywords_found >= 1
                        
                        if is_relevant:
                            threshold_result["relevant_answers"] += 1
                    
                    threshold_result["question_details"].append({
                        "question": question,
                        "answered": answered,
                        "documents_count": len(documents),
                        "relevant": answered and is_relevant if answered else False
                    })
                    
                except Exception as e:
                    logger.error(f"ì„ê³„ê°’ {threshold} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                    threshold_result["question_details"].append({
                        "question": question,
                        "answered": False,
                        "error": str(e)
                    })
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            total = threshold_result["questions_total"]
            answered = threshold_result["questions_answered"]
            relevant = threshold_result["relevant_answers"]
            
            threshold_result["answer_rate"] = answered / total if total > 0 else 0
            threshold_result["precision"] = relevant / answered if answered > 0 else 0
            threshold_result["recall"] = relevant / total if total > 0 else 0
            
            # F1 ìŠ¤ì½”ì–´
            p = threshold_result["precision"]
            r = threshold_result["recall"]
            threshold_result["f1_score"] = 2 * p * r / (p + r) if (p + r) > 0 else 0
            
            performance_results["threshold_results"].append(threshold_result)
        
        return performance_results
    
    def save_results(self, results: Dict, filename: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_file = project_root / "logs" / filename
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” ìœ ì‚¬ë„ ì ìˆ˜ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    analyzer = SimilarityAnalyzer()
    
    # 1. ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„
    print("\n1ï¸âƒ£ ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...")
    similarity_results = await analyzer.analyze_question_document_similarity()
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    analyzer.save_results(similarity_results, f"similarity_analysis_{timestamp}.json")
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    stats = similarity_results.get("overall_stats", {})
    if stats:
        print(f"\nğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜ í†µê³„:")
        print(f"  - ìµœëŒ€ ì ìˆ˜: {stats['max_score']:.6f}")
        print(f"  - í‰ê·  ì ìˆ˜: {stats['avg_score']:.6f}")
        print(f"  - ì¤‘ê°„ ì ìˆ˜: {stats['median_score']:.6f}")
        print(f"  - ìµœì†Œ ì ìˆ˜: {stats['min_score']:.6f}")
        
        recommendations = similarity_results.get("recommendations", {})
        if recommendations:
            print(f"\nğŸ’¡ ì„ê³„ê°’ ì¶”ì²œ:")
            print(f"  - í˜„ì¬ ì„¤ì •: {recommendations['current_threshold']}")
            print(f"  - ì¶”ì²œ ê°’: {recommendations['suggested_threshold']}")
            print(f"  - ë³´ìˆ˜ì : {recommendations['conservative_threshold']}")
            print(f"  - ê· í˜•ì : {recommendations['balanced_threshold']}")
    
    # 2. ì„ê³„ê°’ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print(f"\n2ï¸âƒ£ ì„ê³„ê°’ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
    test_thresholds = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05]
    performance_results = await analyzer.test_threshold_performance(test_thresholds)
    
    analyzer.save_results(performance_results, f"threshold_performance_{timestamp}.json")
    
    # ì„±ëŠ¥ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“ˆ ì„ê³„ê°’ë³„ ì„±ëŠ¥:")
    for result in performance_results["threshold_results"]:
        threshold = result["threshold"]
        answer_rate = result["answer_rate"] * 100
        f1_score = result["f1_score"]
        print(f"  - {threshold:5.3f}: ë‹µë³€ë¥  {answer_rate:5.1f}%, F1 {f1_score:.3f}")
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” logs/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main()) 