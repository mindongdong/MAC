#!/usr/bin/env python3
# scripts/evaluate_thresholds.py

"""
RAG ì„ê³„ê°’ ìµœì í™” í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- ë‹¤ì–‘í•œ MIN_RELEVANCE_SCORE ê°’ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€
- Golden Set ê¸°ë°˜ ì •í™•ë„ ì¸¡ì •
"""

import asyncio
import sys
import os
from pathlib import Path
import json
import logging
import time
from typing import List, Dict, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from app.services.langchain_service import LangChainService
from app.config import settings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ThresholdEvaluator:
    def __init__(self):
        self.langchain_service = LangChainService()
        
        # Golden Set: í‰ê°€ìš© ì§ˆë¬¸ê³¼ ê¸°ëŒ€ë˜ëŠ” í‚¤ì›Œë“œë“¤
        self.golden_set = [
            {
                "question": "ë Œì˜ ì£¼ë ¥ ìŠ¤í‚¬ì€ ë­ì•¼?",
                "expected_keywords": ["ë Œ", "ìŠ¤í‚¬", "ì£¼ë ¥"],
                "category": "class_skill"
            },
            {
                "question": "ì±Œë¦°ì €ìŠ¤ ì½”ì¸ìƒµì—ì„œ ë­˜ ì‚´ ìˆ˜ ìˆì–´?",
                "expected_keywords": ["ì±Œë¦°ì €ìŠ¤", "ì½”ì¸ìƒµ", "êµ¬ë§¤"],
                "category": "system_shop"
            },
            {
                "question": "í•˜ì´í¼ ë²„ë‹ ì´ë²¤íŠ¸ ë³´ìƒì€?",
                "expected_keywords": ["í•˜ì´í¼", "ë²„ë‹", "ì´ë²¤íŠ¸", "ë³´ìƒ"],
                "category": "event_reward"
            },
            {
                "question": "ì†” ì—ë¥´ë‹¤ëŠ” ì–´ë–»ê²Œ ì–»ì–´?",
                "expected_keywords": ["ì†”", "ì—ë¥´ë‹¤", "íšë“"],
                "category": "item_obtain"
            },
            {
                "question": "ë©”ì´í”Œ íŒ¨ìŠ¤ í˜œíƒì´ ë­ì•¼?",
                "expected_keywords": ["ë©”ì´í”Œ", "íŒ¨ìŠ¤", "í˜œíƒ"],
                "category": "system_benefit"
            },
            {
                "question": "ë³´ìŠ¤ ëª¬ìŠ¤í„° ì¶”ì²œí•´ì¤˜",
                "expected_keywords": ["ë³´ìŠ¤", "ëª¬ìŠ¤í„°", "ì¶”ì²œ"],
                "category": "boss_guide"
            },
            {
                "question": "ë§í¬ ìŠ¤í‚¬ ì¶”ì²œ",
                "expected_keywords": ["ë§í¬", "ìŠ¤í‚¬"],
                "category": "character_system"
            },
            {
                "question": "ì‹ ê·œ ìœ ì € ê°€ì´ë“œ",
                "expected_keywords": ["ì‹ ê·œ", "ìœ ì €", "ê°€ì´ë“œ", "ì´ˆë³´"],
                "category": "beginner_guide"
            }
        ]

    async def evaluate_threshold(self, threshold: float) -> Dict:
        """íŠ¹ì • ì„ê³„ê°’ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€"""
        logger.info(f"Evaluating threshold: {threshold}")
        
        # ì„ì‹œë¡œ ì„¤ì • ë³€ê²½
        original_threshold = settings.min_relevance_score
        original_search_type = settings.search_type
        settings.min_relevance_score = threshold
        settings.search_type = "similarity_score_threshold"  # ëª…ì‹œì ìœ¼ë¡œ ì„ê³„ê°’ ê²€ìƒ‰ ì‚¬ìš©
        
        # ìƒˆë¡œìš´ retriever ìƒì„± (ì„¤ì • ë³€ê²½ ë°˜ì˜)
        new_retriever = self.langchain_service.vector_store.get_retriever(
            k=settings.max_retrieval_docs,
            search_type=settings.search_type
        )
        
        # ê¸°ì¡´ retriever ë°±ì—… ë° êµì²´
        original_retriever = self.langchain_service.retriever
        self.langchain_service.retriever = new_retriever
        
        results = {
            "threshold": threshold,
            "total_questions": len(self.golden_set),
            "answered": 0,
            "no_answer": 0,
            "relevant_answers": 0,
            "detailed_results": []
        }
        
        for item in self.golden_set:
            question = item["question"]
            expected_keywords = item["expected_keywords"]
            category = item["category"]
            
            try:
                # ì§ˆë¬¸ ì²˜ë¦¬
                start_time = time.time()
                response_data = await self.langchain_service.chat(
                    message=question,
                    session_id="threshold_evaluator"
                )
                processing_time = time.time() - start_time
                
                response = response_data.get("response", "")
                sources_count = response_data.get("sources_count", 0)
                metadata = response_data.get("metadata", {})
                
                # ë‹µë³€ ë¶„ì„
                is_answered = not ("ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in response or "ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" in response)
                is_relevant = self._check_relevance(response, expected_keywords)
                
                if is_answered:
                    results["answered"] += 1
                    if is_relevant:
                        results["relevant_answers"] += 1
                else:
                    results["no_answer"] += 1
                
                # ìƒì„¸ ê²°ê³¼ ì €ì¥
                detailed_result = {
                    "question": question,
                    "category": category,
                    "answered": is_answered,
                    "relevant": is_relevant,
                    "sources_count": sources_count,
                    "processing_time": round(processing_time, 2),
                    "response_length": len(response),
                    "metadata": metadata
                }
                results["detailed_results"].append(detailed_result)
                
                logger.info(f"Q: {question[:30]}... | Answered: {is_answered} | Relevant: {is_relevant} | Sources: {sources_count}")
                
            except Exception as e:
                logger.error(f"Error processing question '{question}': {str(e)}")
                results["detailed_results"].append({
                    "question": question,
                    "category": category,
                    "answered": False,
                    "relevant": False,
                    "error": str(e)
                })
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        if results["answered"] > 0:
            results["precision"] = results["relevant_answers"] / results["answered"]
        else:
            results["precision"] = 0
            
        results["recall"] = results["answered"] / results["total_questions"]
        
        if results["precision"] + results["recall"] > 0:
            results["f1_score"] = 2 * (results["precision"] * results["recall"]) / (results["precision"] + results["recall"])
        else:
            results["f1_score"] = 0
        
        # ì„¤ì • ë° retriever ë³µì›
        settings.min_relevance_score = original_threshold
        settings.search_type = original_search_type
        self.langchain_service.retriever = original_retriever
        
        return results

    def _check_relevance(self, response: str, expected_keywords: List[str]) -> bool:
        """ë‹µë³€ì˜ ê´€ë ¨ì„± í™•ì¸"""
        response_lower = response.lower()
        
        # ìµœì†Œ ì ˆë°˜ ì´ìƒì˜ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ì•¼ ê´€ë ¨ì„± ìˆë‹¤ê³  íŒë‹¨
        matched_keywords = 0
        for keyword in expected_keywords:
            if keyword.lower() in response_lower:
                matched_keywords += 1
        
        return matched_keywords >= len(expected_keywords) * 0.5

    async def compare_thresholds(self, thresholds: List[float]) -> Dict:
        """ì—¬ëŸ¬ ì„ê³„ê°’ ë¹„êµ í‰ê°€"""
        logger.info(f"Comparing thresholds: {thresholds}")
        
        comparison_results = {
            "thresholds": thresholds,
            "results": [],
            "best_threshold": None,
            "best_f1_score": 0
        }
        
        for threshold in thresholds:
            result = await self.evaluate_threshold(threshold)
            comparison_results["results"].append(result)
            
            # ìµœê³  F1 ìŠ¤ì½”ì–´ ì¶”ì 
            if result["f1_score"] > comparison_results["best_f1_score"]:
                comparison_results["best_f1_score"] = result["f1_score"]
                comparison_results["best_threshold"] = threshold
        
        return comparison_results

    def save_results(self, results: Dict, filename: str = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"threshold_evaluation_{timestamp}.json"
        
        results_dir = project_root / "logs"
        results_dir.mkdir(exist_ok=True)
        
        file_path = results_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Results saved to: {file_path}")
        return file_path

    def print_summary(self, results: Dict):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ì„ê³„ê°’ í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        if "results" in results:
            # ì—¬ëŸ¬ ì„ê³„ê°’ ë¹„êµ ê²°ê³¼
            print(f"í‰ê°€ëœ ì„ê³„ê°’: {results['thresholds']}")
            print(f"ìµœì  ì„ê³„ê°’: {results['best_threshold']} (F1: {results['best_f1_score']:.3f})")
            print("\nìƒì„¸ ê²°ê³¼:")
            print(f"{'ì„ê³„ê°’':<8} {'ì‘ë‹µë¥ ':<8} {'ì •í™•ë„':<8} {'F1 ì ìˆ˜':<8}")
            print("-" * 40)
            
            for result in results["results"]:
                print(f"{result['threshold']:<8} {result['recall']:<8.3f} {result['precision']:<8.3f} {result['f1_score']:<8.3f}")
        
        else:
            # ë‹¨ì¼ ì„ê³„ê°’ ê²°ê³¼
            print(f"ì„ê³„ê°’: {results['threshold']}")
            print(f"ì´ ì§ˆë¬¸ ìˆ˜: {results['total_questions']}")
            print(f"ë‹µë³€í•œ ì§ˆë¬¸: {results['answered']}")
            print(f"ë‹µë³€ ê±°ë¶€: {results['no_answer']}")
            print(f"ê´€ë ¨ì„± ìˆëŠ” ë‹µë³€: {results['relevant_answers']}")
            print(f"ì‘ë‹µë¥  (Recall): {results['recall']:.3f}")
            print(f"ì •í™•ë„ (Precision): {results['precision']:.3f}")
            print(f"F1 ì ìˆ˜: {results['f1_score']:.3f}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    evaluator = ThresholdEvaluator()
    
    print("=== RAG ì„ê³„ê°’ ìµœì í™” í‰ê°€ ë„êµ¬ ===")
    print("1. ë‹¨ì¼ ì„ê³„ê°’ í‰ê°€")
    print("2. ì—¬ëŸ¬ ì„ê³„ê°’ ë¹„êµ (ì¶”ì²œ)")
    print("3. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (0.5, 0.6, 0.7)")
    print("4. ë‚®ì€ ì„ê³„ê°’ í…ŒìŠ¤íŠ¸ (0.001, 0.01, 0.1)")
    print("5. ì¢…ë£Œ")
    
    while True:
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-5): ").strip()
        
        if choice == '1':
            threshold = float(input("í‰ê°€í•  ì„ê³„ê°’ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 0.6): "))
            result = await evaluator.evaluate_threshold(threshold)
            evaluator.print_summary(result)
            
            save = input("ê²°ê³¼ë¥¼ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if save == 'y':
                evaluator.save_results(result)
        
        elif choice == '2':
            thresholds_input = input("í‰ê°€í•  ì„ê³„ê°’ë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 0.5,0.6,0.7): ")
            thresholds = [float(x.strip()) for x in thresholds_input.split(',')]
            
            result = await evaluator.compare_thresholds(thresholds)
            evaluator.print_summary(result)
            
            save = input("ê²°ê³¼ë¥¼ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if save == 'y':
                evaluator.save_results(result)
        
        elif choice == '3':
            thresholds = [0.5, 0.6, 0.7]
            result = await evaluator.compare_thresholds(thresholds)
            evaluator.print_summary(result)
            
            save = input("ê²°ê³¼ë¥¼ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if save == 'y':
                evaluator.save_results(result)
        
        elif choice == '4':
            print("í˜„ì¬ ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ë˜ëŠ” ì‹¤ì œ ì ìˆ˜ ë²”ìœ„ í…ŒìŠ¤íŠ¸...")
            thresholds = [0.001, 0.01, 0.1]
            result = await evaluator.compare_thresholds(thresholds)
            evaluator.print_summary(result)
            
            save = input("ê²°ê³¼ë¥¼ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if save == 'y':
                evaluator.save_results(result)
        
        elif choice == '5':
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-5 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    asyncio.run(main()) 