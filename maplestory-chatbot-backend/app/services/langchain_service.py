from langchain_anthropic import ChatAnthropic
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import AsyncCallbackHandler
from langchain.schema import Document
from typing import Dict, List, Optional, AsyncGenerator
import asyncio
from app.config import settings
from app.chains.qa_chain import create_qa_chain
from app.services.vector_store import VectorStoreService
from app.services.embedding_service import get_embeddings
from app.chains.prompts import MAPLESTORY_ANSWER_TEMPLATE
import logging
import re
import os
import json
from difflib import SequenceMatcher
import yaml

logger = logging.getLogger(__name__)

class StreamingCallbackHandler(AsyncCallbackHandler):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬"""
    def __init__(self, queue: asyncio.Queue):
        self.queue = queue
    
    async def on_llm_new_token(self, token: str, **kwargs):
        await self.queue.put(token)
    
    async def on_llm_end(self, response, **kwargs):
        await self.queue.put(None)  # ì¢…ë£Œ ì‹ í˜¸

class LangChainService:
    def __init__(self):
        self.llm = self._initialize_llm()
        self.embeddings = self._initialize_embeddings()
        self.vector_store = VectorStoreService(self.embeddings)
        self.chat_histories: Dict[str, BaseChatMessageHistory] = {}  # ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
        self.retriever = None  # retrieverë¥¼ ë³„ë„ë¡œ ì €ì¥
        
    def _initialize_llm(self):
        """Claude LLM ì´ˆê¸°í™”"""
        return ChatAnthropic(
            anthropic_api_key=settings.anthropic_api_key,
            model=settings.claude_model,
            max_tokens=settings.max_tokens,
            temperature=settings.temperature,
            streaming=True,
        )
    
    def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” - ìœ ì—°í•œ ë°©ì‹"""
        logger.info(f"Initializing embeddings with provider: {settings.get_embedding_provider()}")
        return get_embeddings()
    
    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:
        """ì„¸ì…˜ë³„ ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬ - ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ"""
        if session_id not in self.chat_histories:
            self.chat_histories[session_id] = ChatMessageHistory()
        return self.chat_histories[session_id]
    
    def _apply_answer_template(self, answer: str) -> str:
        """ë‹µë³€ í…œí”Œë¦¿ ì ìš©"""
        if not settings.enable_answer_template:
            return answer
        
        # ì œëª© ì¶”ì¶œ (ì²« ë²ˆì§¸ ## ë˜ëŠ” # ì œëª©)
        title_match = re.search(r'^#{1,2}\s*(.+?)$', answer, re.MULTILINE)
        main_title = title_match.group(1) if title_match else "ë©”ì´í”Œ ê°€ì´ë“œ ë‹µë³€"
        
        # êµ¬ì¡°í™”ëœ ë‚´ìš© (ì œëª© ì œê±°í•œ ë‚˜ë¨¸ì§€)
        structured_content = re.sub(r'^#{1,2}\s*.+?$', '', answer, count=1, flags=re.MULTILINE).strip()
        
        # ì¶”ê°€ íŒ ì¶”ì¶œ (ğŸ’¡, ğŸ“Œ, âš ï¸ ë“±ì˜ ì´ëª¨ì§€ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„)
        tip_pattern = r'(ğŸ’¡|ğŸ“Œ|âš ï¸|ğŸ”¥).*?(?=\n\n|$)'
        tips = re.findall(tip_pattern, structured_content, re.DOTALL)
        additional_tips = "\n".join(tips) if tips else ""
        
        # íŒ ë¶€ë¶„ ì œê±°
        for tip in tips:
            structured_content = structured_content.replace(tip, "").strip()
        
        # ë§ˆë¬´ë¦¬ ë¬¸êµ¬
        closing = "ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ®"
        
        # í…œí”Œë¦¿ ì ìš©
        try:
            return MAPLESTORY_ANSWER_TEMPLATE.format(
                main_title=main_title,
                structured_content=structured_content,
                additional_tips=additional_tips,
                closing=closing
            )
        except Exception as e:
            logger.warning(f"Answer template application failed: {e}")
            return answer  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    async def chat(
        self, 
        message: str, 
        session_id: str,
        context: Optional[Dict] = None,
        stream: bool = False
    ) -> Dict:
        """ì±„íŒ… ì²˜ë¦¬ ë©”ì¸ ë©”ì„œë“œ - ê°œì„ ëœ ë¬¸ì„œ ê²€ì¦ ì ìš©"""
        try:
            # retriever ì´ˆê¸°í™” (ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•Šê³  ì¬ì‚¬ìš©)
            if not self.retriever:
                self.retriever = self.vector_store.get_retriever(
                    k=settings.max_retrieval_docs,
                    search_type=settings.search_type
                )
            
            # QA ì²´ì¸ ìƒì„± (ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜)
            qa_chain = create_qa_chain(
                llm=self.llm,
                retriever=self.retriever
            )
            
            # ë©”ëª¨ë¦¬ê°€ ìˆëŠ” ì²´ì¸ìœ¼ë¡œ ë˜í•‘
            chain_with_history = RunnableWithMessageHistory(
                qa_chain,
                self.get_session_history,
                input_messages_key="input",
                history_messages_key="chat_history",
            )
            
            if stream:
                return await self._stream_chat(chain_with_history, message, session_id, context)
            else:
                return await self._enhanced_regular_chat(chain_with_history, message, session_id, context)
                
        except Exception as e:
            logger.error(f"Chat error: {str(e)}")
            raise
    
    async def _enhanced_regular_chat(self, qa_chain, message: str, session_id: str, context: Optional[Dict]) -> Dict:
        """ê°œì„ ëœ ì¼ë°˜ ì±„íŒ… ì²˜ë¦¬ - ë¬¸ì„œ ê²€ì¦ ë° ë‹µë³€ í›„ì²˜ë¦¬ í¬í•¨"""
        
        # 1. ë¨¼ì € ë¬¸ì„œ ê²€ìƒ‰ (retrieverë¥¼ ì§ì ‘ ì‚¬ìš©)
        raw_documents = await self.retriever.aget_relevant_documents(message)
        
        # 2. ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦
        validated_documents = self._validate_document_relevance(raw_documents, message)
        
        logger.info(f"Documents: {len(raw_documents)} -> {len(validated_documents)} (after validation)")
        
        if not validated_documents:
            # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê±°ë‚˜ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.",
                "sources": [],
                "metadata": {
                    "model": settings.claude_model,
                    "tokens_used": 0,
                    "sources_count": 0,
                    "valid_sources_count": 0,
                    "system_prompt_used": settings.use_system_prompt,
                    "template_applied": settings.enable_answer_template,
                    "documents_filtered": len(raw_documents),
                    "relevance_check": "no_relevant_documents"
                }
            }
        
        # 3. ê²€ì¦ëœ ë¬¸ì„œë¡œ QA ìˆ˜í–‰ (ì„¸ì…˜ íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜)
        response = await qa_chain.ainvoke(
            {"input": message},
            config={"configurable": {"session_id": session_id}}
        )
        
        # 4. ë‹µë³€ í›„ì²˜ë¦¬ ê²€ì¦
        raw_answer = response["answer"]
        validated_answer = self._validate_response(raw_answer, validated_documents, message)
        
        # 5. ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
        sources = self._extract_sources(validated_documents)
        
        # 6. ë‹µë³€ì— í…œí”Œë¦¿ ì ìš©
        formatted_response = self._apply_answer_template(validated_answer)
        
        # 7. ì°¸ê³ ìë£Œë¥¼ URLë§Œ ì¶œë ¥í•˜ë„ë¡ ê°„ì†Œí™”
        sources_with_urls = [s for s in sources if s.get('has_url') and s.get('url')]
        if sources_with_urls and settings.require_url_in_sources:
            formatted_response += "\n\n## ğŸ“š ì°¸ê³ ìë£Œ\n"
            for source in sources_with_urls[:settings.max_reference_sources]:
                url = source.get('url', '')
                
                if url.startswith(('http://', 'https://', 'www.')):
                    # ê°„ë‹¨í•œ í˜•ì‹: URLë§Œ í‘œì‹œ
                    formatted_response += f"* {url}\n"
        
        return {
            "response": formatted_response,
            "sources": sources,
            "metadata": {
                "model": settings.claude_model,
                "tokens_used": response.get("tokens_used", 0),
                "sources_count": len(sources),
                "valid_sources_count": len(sources_with_urls),
                "system_prompt_used": settings.use_system_prompt,
                "template_applied": settings.enable_answer_template,
                "documents_filtered": len(raw_documents) - len(validated_documents),
                "original_documents": len(raw_documents),
                "validated_documents": len(validated_documents),
                "relevance_check": "passed",
                "response_length": len(formatted_response)
            }
        }
    
    async def _stream_chat(
        self, 
        qa_chain, 
        message: str, 
        session_id: str,
        context: Optional[Dict]
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì²˜ë¦¬"""
        try:
            # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ í ìƒì„±
            queue = asyncio.Queue()
            
            # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
            streaming_handler = StreamingCallbackHandler(queue)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            async def stream_response():
                try:
                    response = await qa_chain.ainvoke(
                        {"input": message},
                        config={
                            "configurable": {"session_id": session_id},
                            "callbacks": [streaming_handler]
                        }
                    )
                except Exception as e:
                    await queue.put(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    await queue.put(None)
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‘ë‹µ ìƒì„± ì‹œì‘
            task = asyncio.create_task(stream_response())
            
            # ìŠ¤íŠ¸ë¦¬ë° í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ yield
            while True:
                token = await queue.get()
                if token is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                yield token
            
            # íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
            await task
            
        except Exception as e:
            logger.error(f"Streaming chat error: {str(e)}")
            yield f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _extract_sources(self, documents: List[Document]) -> List[Dict]:
        """ê°œì„ ëœ ì†ŒìŠ¤ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ - sources ë©”íƒ€ë°ì´í„° íŒŒì‹± í¬í•¨"""
        sources = []
        seen_sources = set()  # ì¤‘ë³µ ì œê±°
        
        for doc in documents[:5]:  # ìƒìœ„ 5ê°œë¡œ í™•ì¥
            # ê³ ìœ  ì‹ë³„ì ìƒì„±
            source_id = f"{doc.metadata.get('source', 'Unknown')}_{doc.metadata.get('chunk_index', 0)}"
            
            if source_id not in seen_sources:
                seen_sources.add(source_id)
                
                # ë¬¸ì„œì—ì„œ sources ë©”íƒ€ë°ì´í„° íŒŒì‹±
                sources_info = self._parse_sources_from_document(doc.page_content)
                
                # URL, title, creator ì¶”ì¶œ
                url = None
                creator = None
                doc_title = None
                
                # 1. íŒŒì‹±ëœ sources ì •ë³´ ì‚¬ìš© (ìš°ì„ ìˆœìœ„)
                if sources_info:
                    first_source = sources_info[0]  # ì²« ë²ˆì§¸ source ì‚¬ìš©
                    url = first_source.get('url')
                    creator = first_source.get('creator')
                    doc_title = first_source.get('title')
                
                # 2. ë©”íƒ€ë°ì´í„°ì—ì„œ ëŒ€ì²´ê°’ ì°¾ê¸°
                if not url:
                    url = doc.metadata.get("url", None)
                if not creator:
                    creator = doc.metadata.get("author", None)
                if not doc_title:
                    doc_title = doc.metadata.get("title", None)
                
                # 3. ë¬¸ì„œ ë‚´ìš©ì—ì„œ URL íŒ¨í„´ ì°¾ê¸° (fallback)
                if not url:
                    markdown_link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
                    link_matches = re.findall(markdown_link_pattern, doc.page_content)
                    if link_matches:
                        url = link_matches[0][1]
                
                if not url:
                    url_pattern = r'URL:\s*`?([^`\s\n]+)`?'
                    url_match = re.search(url_pattern, doc.page_content)
                    if url_match:
                        url = url_match.group(1)
                
                # 4. ì œëª© ì¶”ì¶œ (fallback)
                if not doc_title:
                    header_pattern = r'^#{1,3}\s+(.+)$'
                    header_match = re.search(header_pattern, doc.page_content, re.MULTILINE)
                    if header_match:
                        doc_title = header_match.group(1).strip()
                    else:
                        source_filename = doc.metadata.get("source", "Unknown")
                        doc_title = os.path.splitext(source_filename)[0]
                
                sources.append({
                    "title": doc_title,
                    "creator": creator or "Unknown",
                    "category": doc.metadata.get("category", "N/A"),
                    "author": doc.metadata.get("author", creator or "Unknown"),
                    "section": doc.metadata.get("section", "N/A"),
                    "chunk_index": doc.metadata.get("chunk_index", 0),
                    "relevance_score": doc.metadata.get("score", 0),
                    "preview": doc.page_content[:150] + "...",
                    "url": url,
                    "has_url": bool(url)
                })
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        sources.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        return sources
    
    def _parse_sources_from_document(self, content: str) -> List[Dict]:
        """ë¬¸ì„œ ë‚´ìš©ì—ì„œ sources ë©”íƒ€ë°ì´í„° íŒŒì‹±"""
        try:
            # YAML front matter ì°¾ê¸°
            yaml_pattern = r'^---\s*\n(.*?)\n---'
            yaml_match = re.search(yaml_pattern, content, re.DOTALL | re.MULTILINE)
            
            if yaml_match:
                yaml_content = yaml_match.group(1)
                try:
                    # YAML íŒŒì‹±
                    metadata = yaml.safe_load(yaml_content)
                    if isinstance(metadata, dict) and 'sources' in metadata:
                        sources_list = metadata['sources']
                        if isinstance(sources_list, list):
                            parsed_sources = []
                            for source in sources_list:
                                if isinstance(source, dict):
                                    parsed_sources.append({
                                        'url': source.get('url', ''),
                                        'title': source.get('title', ''),
                                        'creator': source.get('creator', '')
                                    })
                            return parsed_sources
                except yaml.YAMLError:
                    logger.debug("YAML parsing failed for document sources")
            
            return []
            
        except Exception as e:
            logger.debug(f"Error parsing sources from document: {e}")
            return []
    
    async def add_documents(self, documents: List[Document]):
        """ë¬¸ì„œ ì¶”ê°€"""
        await self.vector_store.add_documents(documents)
    
    def clear_memory(self, session_id: str):
        """íŠ¹ì • ì„¸ì…˜ì˜ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        if session_id in self.chat_histories:
            self.chat_histories[session_id].clear()
            logger.info(f"Cleared memory for session: {session_id}")
    
    def _extract_keywords(self, query: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë©”ì´í”ŒìŠ¤í† ë¦¬ íŠ¹í™” í‚¤ì›Œë“œ íŒ¨í„´
        maple_patterns = [
            r'[ê°€-í£]+\s*(?:ë³´ìŠ¤|ìŠ¤í‚¬|íë¸Œ|ì½”ì¸|í¬ì¸íŠ¸|ì´ë²¤íŠ¸|ì§ì—…|í´ë˜ìŠ¤)',
            r'(?:í•˜ë“œ|ì´ì§€|í—¬|ì¹´ì˜¤ìŠ¤)\s*[ê°€-í£]+',
            r'[ê°€-í£]+\s*(?:ìƒµ|ìƒì )',
            r'ì±Œë¦°ì €ìŠ¤?\s*[ê°€-í£]*',
            r'[ê°€-í£]{2,}(?:ë Œ|ì œë¡œ|ì¹´ë°ë‚˜|ì¼ë¦¬ì›€|í˜¸ì˜|ì•„ë¸|ì¹´ì¸|ë¼ë¼|ì•„í¬)',
        ]
        
        keywords = []
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        for pattern in maple_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            keywords.extend(matches)
        
        # ì¼ë°˜ ëª…ì‚¬ ì¶”ì¶œ (2ê¸€ì ì´ìƒ í•œê¸€)
        general_keywords = re.findall(r'[ê°€-í£]{2,}', query)
        keywords.extend(general_keywords)
        
        # ì¤‘ë³µ ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        unique_keywords = list(set([kw.strip().lower() for kw in keywords if len(kw.strip()) > 1]))
        
        return unique_keywords[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
    
    def _calculate_title_relevance(self, title: str, query: str) -> float:
        """ë¬¸ì„œ ì œëª©ê³¼ ì§ˆë¬¸ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not title:
            return 0.0
            
        title_lower = title.lower()
        query_lower = query.lower()
        
        # ì§ì ‘ ë§¤ì¹­ ì ìˆ˜
        direct_match = SequenceMatcher(None, title_lower, query_lower).ratio()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        query_keywords = self._extract_keywords(query)
        title_keywords = self._extract_keywords(title)
        
        keyword_matches = 0
        for q_kw in query_keywords:
            for t_kw in title_keywords:
                if q_kw in t_kw or t_kw in q_kw:
                    keyword_matches += 1
                    break
        
        keyword_score = keyword_matches / max(len(query_keywords), 1)
        
        # ìµœì¢… ì ìˆ˜ (ì§ì ‘ ë§¤ì¹­ 70% + í‚¤ì›Œë“œ ë§¤ì¹­ 30%)
        final_score = (direct_match * 0.7) + (keyword_score * 0.3)
        
        return min(final_score, 1.0)
    
    def _validate_document_relevance(self, documents: List[Document], query: str) -> List[Document]:
        """ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦ ë° í•„í„°ë§"""
        if not settings.enable_document_filtering:
            logger.info(f"Document filtering disabled - returning {len(documents)} documents as-is")
            return documents[:settings.max_reference_sources]
        
        validated_docs = []
        query_keywords = self._extract_keywords(query)
        
        for doc in documents:
            # 1. ì œëª© ê´€ë ¨ì„± ê²€ì‚¬
            title = doc.metadata.get('title', '')
            title_relevance = self._calculate_title_relevance(title, query)
            
            # 2. ë‚´ìš© í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ì‚¬
            content_lower = doc.page_content.lower()
            keyword_matches = sum(1 for kw in query_keywords if kw in content_lower)
            content_relevance = keyword_matches / max(len(query_keywords), 1)
            
            # 3. ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê´€ë ¨ì„± ê²€ì‚¬
            category = doc.metadata.get('category', '')
            class_name = doc.metadata.get('class', '')
            
            # ì§ì—…ëª…ì´ ì§ˆë¬¸ì— ìˆëŠ” ê²½ìš° í•´ë‹¹ ì§ì—… ë¬¸ì„œ ìš°ì„ 
            metadata_bonus = 0.0
            for keyword in query_keywords:
                if keyword in class_name.lower():
                    metadata_bonus += 0.3
                if keyword in category.lower():
                    metadata_bonus += 0.2
            
            # ìµœì¢… ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            final_relevance = (title_relevance * 0.4) + (content_relevance * 0.4) + metadata_bonus
            
            # ì„ê³„ê°’ ì´ìƒì¸ ë¬¸ì„œë§Œ í¬í•¨
            if final_relevance >= 0.3:  # 30% ì´ìƒ ê´€ë ¨ì„±
                doc.metadata['relevance_score'] = final_relevance
                validated_docs.append(doc)
                
                logger.debug(f"Document accepted: {title} (relevance: {final_relevance:.2f})")
            else:
                logger.debug(f"Document filtered out: {title} (relevance: {final_relevance:.2f})")
        
        # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        validated_docs.sort(key=lambda x: x.metadata.get('relevance_score', 0), reverse=True)
        
        return validated_docs[:settings.max_reference_sources]
    
    def _validate_response(self, response: str, documents: List[Document], query: str) -> str:
        """ë‹µë³€ í›„ì²˜ë¦¬ ê²€ì¦ - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€"""
        if not settings.enable_response_validation:
            return response
        
        validated_response = response
        
        # 1. ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ê²€ì‚¬
        suspicious_patterns = [
            (r'\d+ê°œ(?:\s*(?:ì˜|ë¥¼|ì„|ì´|ê°€))?', 'êµ¬ì²´ì  ìˆ˜ëŸ‰'),
            (r'[ê°€-í£]+\s*íë¸Œ', 'íë¸Œ ì•„ì´í…œ'),
            (r'[ê°€-í£]+\s*ìŠ¤í‚¬', 'ìŠ¤í‚¬ëª…'),
            (r'\d+(?:,\d{3})*\s*(?:ë©”ì†Œ|í¬ì¸íŠ¸|ì ìˆ˜)', 'ìˆ˜ì¹˜ ì •ë³´'),
            (r'(?:ëŒ€ëµ|ì•½|ìˆ˜ì‹­ì–µ|ë‹¤ìˆ˜|ì—¬ëŸ¬|ì¼ë°˜ì ìœ¼ë¡œ|ë³´í†µ)', 'ëª¨í˜¸í•œ í‘œí˜„'),
        ]
        
        warnings = []
        
        for pattern, description in suspicious_patterns:
            matches = re.findall(pattern, validated_response, re.IGNORECASE)
            for match in matches:
                # ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš© í™•ì¸
                if not self._verify_in_documents(match, documents):
                    # ëª¨í˜¸í•œ í‘œí˜„ì€ ì œê±°
                    if description == 'ëª¨í˜¸í•œ í‘œí˜„':
                        validated_response = re.sub(pattern, '', validated_response, flags=re.IGNORECASE)
                        warnings.append(f"ëª¨í˜¸í•œ í‘œí˜„ '{match}' ì œê±°ë¨")
                    else:
                        warnings.append(f"ê²€ì¦ë˜ì§€ ì•Šì€ {description}: '{match}'")
        
        # 2. ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ê²€ì‚¬
        query_keywords = self._extract_keywords(query)
        response_keywords = self._extract_keywords(validated_response)
        
        # ë‹µë³€ì˜ í‚¤ì›Œë“œê°€ ì§ˆë¬¸ê³¼ ë„ˆë¬´ ë™ë–¨ì–´ì§„ ê²½ìš°
        keyword_overlap = len(set(query_keywords) & set(response_keywords))
        if keyword_overlap == 0 and len(query_keywords) > 0:
            warnings.append("ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ì„ ìˆ˜ ìˆìŒ")
        
        # ê²½ê³ ê°€ ìˆìœ¼ë©´ ë¡œê·¸ì— ê¸°ë¡
        if warnings:
            logger.warning(f"Response validation warnings: {'; '.join(warnings)}")
        
        return validated_response.strip()
    
    def _verify_in_documents(self, text: str, documents: List[Document]) -> bool:
        """í…ìŠ¤íŠ¸ê°€ ì°¸ê³  ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        text_lower = text.lower().strip()
        
        for doc in documents:
            if text_lower in doc.page_content.lower():
                return True
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œë„ í™•ì¸
            for key, value in doc.metadata.items():
                if isinstance(value, str) and text_lower in value.lower():
                    return True
        
        return False 