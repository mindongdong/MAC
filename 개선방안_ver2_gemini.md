이제 안정적으로 작동하는 기반 위에서 다시 품질을 점진적으로 높여나갈 차례입니다. 질문해주신 5가지 항목에 대해 구체적인 전략과 방안을 제시해 드리겠습니다.

### 1. 최적 임계값(`MIN_RELEVANCE_SCORE`) 설정 전략

현재 `0.5`로 설정하여 시스템이 작동하게 만든 것은 훌륭한 응급조치입니다. 이제 이 값을 최적화하여 "관련 없는 문서는 거르되, 관련 있는 문서는 놓치지 않는" 최적의 균형점을 찾아야 합니다.

* **품질과 정확성의 균형점:**
    * **임계값이 낮을 때 (e.g., 0.5):** 검색 결과(Recall)는 높아지지만, 관련 없는 문서가 포함될 확률(Precision 저하)이 높아져 LLM에 노이즈를 제공하고 환각의 원인이 될 수 있습니다.
    * **임계값이 높을 때 (e.g., 0.75):** 검색 정확도(Precision)는 높아지지만, 정말 관련 있는 일부 문서마저 점수가 미달하여 놓칠 수(Recall 저하) 있습니다. 이것이 "답변을 찾을 수 없다"는 문제의 원인이었습니다.
    * **결론:** 최적의 임계값은 "정답을 포함한 문서들의 평균 점수"와 "오답 문서들의 평균 점수" 사이 어딘가에 존재합니다. **일반적으로 잘 튜닝된 RAG 시스템에서는 0.7 ~ 0.8 사이를 목표**로 합니다.

* **단계적 상향 전략 (데이터 기반 접근):**
    1.  **'Golden Set' 평가 데이터 구축:** 먼저 15~20개 정도의 대표적인 질문과, 각 질문에 대한 정답을 포함하는 '정답 문서 ID' 목록을 만듭니다.
        * 예: `{"question": "렌의 주력 스킬은?", "correct_doc_id": "maple_skill_doc_123.md"}`
    2.  **점수 로깅 및 분석:** `SEARCH_TYPE`을 `similarity`로 설정하고, `k=10` 정도로 늘려서 평가용 질문들을 실행합니다. 이때 검색된 모든 문서의 **유사도 점수를 로깅**합니다.
    3.  **임계값 후보군 도출:**
        * '정답 문서'가 검색되었을 때의 점수 분포를 확인합니다. (예: 대부분 0.72 ~ 0.85 사이)
        * '오답 문서'들의 점수 분포를 확인합니다. (예: 대부분 0.68 이하)
        * 이 분석을 통해, 우리 시스템의 임계값은 **0.7** 근처가 적절할 수 있다는 가설을 세울 수 있습니다.
    4.  **점진적 상향 및 테스트:** 현재 `0.5`에서 시작하여, `0.6`, `0.65`, `0.7` 등으로 0.05씩 올려가면서 'Golden Set' 테스트를 반복합니다. 정답 문서가 필터링되지 않고, 오답 문서가 효과적으로 제거되는 지점을 찾습니다.

### 2. 문서 메타데이터 개선

`title: "No Title"` 문제는 검색 품질에 치명적일 수 있습니다. 특히 LLM이 관련성을 판단하는 후처리 과정(`_validate_document_relevance`)에서 문서의 제목은 매우 중요한 정보입니다.

* **문제 해결 방안:** 문서 처리 단계에서 제목 추출 로직을 더 견고하게 만들어야 합니다.
* **개선 방법:** `app/services/document_processor.py`의 문서 처리 로직에 다음 순서로 제목을 추출하는 Fallback 체계를 만듭니다.

    ```python
    # in app/services/document_processor.py

    def _extract_title(self, content: str, file_path: str) -> str:
        """문서 제목을 견고하게 추출하는 로직"""
        
        # 1. YAML Frontmatter에서 'title' 필드 우선 검색
        try:
            from yaml import safe_load
            # --- 로 둘러싸인 부분을 찾아서 파싱
            if content.startswith("---"):
                end_marker = content.find("---", 3)
                if end_marker != -1:
                    yaml_content = content[3:end_marker]
                    metadata = safe_load(yaml_content)
                    if isinstance(metadata, dict) and 'title' in metadata and metadata['title']:
                        return metadata['title']
        except Exception:
            pass # YAML 파싱 실패 시 다음으로 넘어감

        # 2. Markdown의 첫 번째 H1 태그 ('# 제목')을 제목으로 사용
        lines = content.split('\n')
        for line in lines:
            if line.startswith('# '):
                return line[2:].strip()

        # 3. 위 방법들이 모두 실패하면, 파일명을 제목으로 사용
        import os
        return os.path.splitext(os.path.basename(file_path))[0].replace('_', ' ')

    # ... DocumentProcessor의 다른 메소드에서 _extract_title 호출 ...
    # 예시: process_file 메소드 내
    # title = self._extract_title(content, file_path)
    # metadata['title'] = title if title else "Untitled Document"
    ```
    이 로직을 적용한 후, `scripts/ingest_documents.py`를 다시 실행하여 모든 문서의 메타데이터를 갱신해야 합니다.

### 3. 점진적 품질 개선 전략

현재 안정적으로 작동하는 상태에서, 껐던 기능들을 하나씩 다시 켜면서 시스템을 강화해야 합니다. 다음 순서를 추천합니다.

1.  **[1순위] 메타데이터 개선 및 재색인:** 위 2번 항목을 먼저 해결하고 모든 문서에 유효한 제목이 들어가도록 합니다.
2.  **[2순위] 임계값 점진적 상향:** 1번에서 제안한 데이터 기반 접근법으로 최적의 `MIN_RELEVANCE_SCORE`를 찾고 `SEARCH_TYPE`을 `similarity_score_threshold`로 **복구**합니다. (예: `0.7`로 설정)
3.  **[3순위] 문서 사전 필터링(`ENABLE_DOCUMENT_FILTERING`) 재활성화:** 이 기능은 검색된 문서가 정말 질문과 관련 있는지 LLM을 통해 한 번 더 검증하는 역할을 합니다. 비용과 시간이 더 들지만 정확도를 높입니다. 1, 2단계가 안정화된 후에 켜서 효과를 검증합니다.
4.  **[4순위] 답변 후처리 검증(`ENABLE_RESPONSE_VALIDATION`) 재활성화:** LLM이 생성한 최종 답변이 검색된 문서 내용에 기반했는지 마지막으로 확인하는 안전망입니다. 모든 것이 안정적으로 작동할 때 마지막으로 켜서 환각을 최종적으로 억제합니다.

**균형점:** 각 단계를 적용할 때마다 `Golden Set`으로 테스트하여 "정확한 답변을 하는가?"와 "답변을 회피하지는 않는가?"를 모두 확인해야 합니다.

### 4. 검색 전략 최적화

`similarity`, `similarity_score_threshold`, `mmr`은 각각 뚜렷한 장단점을 가집니다.

| 검색 전략                      | 장점                                                                      | 단점                                                              | 메이플스토리 도메인 최적 사용 사례                               |
| :----------------------------- | :------------------------------------------------------------------------ | :---------------------------------------------------------------- | :--------------------------------------------------------------- |
| **`similarity`** | 빠르고 간단함. 질문과 가장 유사한 청크(chunk)를 확실하게 찾아줌.            | 관련성이 낮아도 무조건 `k`개를 반환. 유사하지만 중복된 정보를 줄 수 있음. | 디버깅 및 점수 분포 확인용으로 적합.                               |
| **`similarity_score_threshold`** | **정확도(Precision)가 가장 높음**. 설정된 임계값 이하의 관련 없는 문서를 원천 차단. | 임계값이 너무 높으면 좋은 문서도 놓침(Recall 저하).                | **"렌의 스킬 목록 알려줘", "챌린저스 코인샵 판매 아이템은?"** 등 명확한 정답이 있는 질문에 가장 적합. (현재 목표에 가장 부합) |
| **`mmr` (Maximal Marginal Relevance)** | **다양성 확보에 유리**. 검색 결과가 여러 주제를 포함하도록 하여 풍부한 답변 유도. | 가장 정확한 정보의 순위가 뒤로 밀릴 수 있음.                       | **"이번 여름 업데이트 요약해줘", "신규 유저 가이드라인 알려줘"** 등 여러 정보를 종합해야 하는 개방형 질문에 유리. |

**최적화 전략:**
* **기본값:** `similarity_score_threshold`를 기본으로 사용하되, 임계값은 2번 전략을 통해 찾은 최적의 값(예: `0.7`)으로 설정합니다. 이것이 현재 겪었던 문제들을 해결하는 가장 직접적인 방법입니다.
* **고급 전략 (추후):** 사용자의 질문을 분석하여 의도를 파악하고, 정보 요약형 질문에는 `mmr`을, 사실 확인형 질문에는 `similarity_score_threshold`를 동적으로 적용하는 방식을 고려해볼 수 있습니다.

### 5. 시스템 프롬프트 개선

현재 프롬프트가 너무 제한적이라기보다는, **검색(Retrieval) 단계의 품질이 낮아서** 프롬프트의 엄격한 규칙에 따라 "답변할 수 없음"을 반환했을 가능성이 큽니다. 검색 품질이 개선된 지금, 프롬프트를 약간 수정하여 더 유용한 답변을 유도할 수 있습니다.

* **현재 프롬프트의 역할:** 환각을 방지하는 강력한 가드레일 역할. 이 기조는 **유지**해야 합니다.
* **개선 방향:** 정확성을 해치지 않는 선에서, 답변을 좀 더 사용자 친화적으로 가공하도록 지시를 추가합니다.

    ```python
    # 제안하는 프롬프트 개선안 (일부)

    # ... 기존 프롬프트의 '핵심 원칙', '절대 금지사항'은 유지 ...

    ## ✍️ 답변 생성 가이드라인
    1. **정보 종합 및 요약**: 질문에 대한 답변이 여러 문서에 흩어져 있다면, 이를 종합하여 하나의 일관된 답변으로 구성하세요.
    2. **가독성 높은 형식 사용**: 정보를 단순히 나열하지 말고, 사용자가 이해하기 쉽도록 **리스트나 표**를 적극적으로 활용하여 정리해주세요.
    3. **핵심 정보 강조**: 가장 중요하다고 생각되는 정보는 **볼드체**로 강조하여 사용자가 빠르게 파악할 수 있도록 도와주세요.

    ---
    [참고 문서]
    {context}
    ---
    
    위 모든 규칙과 가이드라인을 반드시 준수하여 다음 질문에 답변하세요.
    ```
    이러한 가이드라인은 LLM이 단순히 정보를 복사-붙여넣기 하는 것을 넘어, 주어진 정보를 바탕으로 '유용한 정리'를 하도록 유도합니다. "추측하지 말라"는 핵심 규칙은 그대로 두었기 때문에 환각의 위험을 높이지 않으면서 답변의 품질을 향상시킬 수 있습니다.
